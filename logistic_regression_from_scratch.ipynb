{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to import the necessary libaries\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to classify the iris dataset, RUN THE NEXT CELL WITHOUT MODIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading and preprocessing the data, don't change\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETE THE CODE IN THE FOLLOWING CELL, THEN RUN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####COMPLETE THE FOLLOWING TWO FUNCTIONS\n",
    "def Gradient(W, X_batch, t_batch):\n",
    "    ### Complete this function to calculate the gradient of the cross entropy loss\n",
    "    Y = softmax(W @ X_batch, axis = 0) #this is our model for the prediction \n",
    "\n",
    "    N = X_batch.shape[1] #number of points in batch\n",
    "    K = W.shape[0] #number of classes\n",
    "    M = W.shape[1] #number of features\n",
    "    \n",
    "    G = np.zeros((K, M)) #this will store the gradient, the dimensions are K: number of classes, M: number of features\n",
    "    \n",
    "    # calculate the gradient\n",
    "    for i in range(N):\n",
    "        x_i = X_batch[:, i].reshape(-1, 1)\n",
    "        t_i = t_batch[:, i].reshape(-1, 1)\n",
    "        y_i = Y[:, i].reshape(-1, 1)\n",
    "\n",
    "        G += np.outer(y_i - t_i, x_i)\n",
    "            \n",
    "    return G\n",
    "\n",
    "def logisticGD(X_train, t_train, batch_size, l_rate = 0.1,tol = 1e-5, epochs = 10):\n",
    "    \"\"\" \n",
    "    batch_size = size of training set, correspond to GD\n",
    "    batch_size = 1, correspond to SGD\n",
    "    all other values correspond to mini batch GD.\n",
    "    \"\"\"\n",
    "    K = t_train.shape[0]\n",
    "    N = X_train.shape[1]\n",
    "    M = X_train.shape[0]\n",
    "    \n",
    "    W = np.random.rand(K, M) #initialization of the model parameters\n",
    "    \n",
    "    norm_G = float('inf')\n",
    "    \n",
    "    n_batches = N // batch_size\n",
    "\n",
    "    epoch = 1\n",
    "    \n",
    "    while epoch <= epochs and norm_G > tol:\n",
    "        indices = np.random.permutation(N)#select a random permutation of N\n",
    "        X_shuffle = X_train[:, indices] #permute the training points according to the chosen permutation\n",
    "        t_shuffle = t_train[:, indices] #permute the training points according to the chosen permutation\n",
    "\n",
    "        for j in range(n_batches):\n",
    "            start = j*batch_size\n",
    "            end = (j+1)*batch_size\n",
    "            X_batch = X_shuffle[:, start:end] #select the batch by slicing between j * batch_size and (j+1)*batch_size in the shuffled data\n",
    "            t_batch = t_shuffle[:, start:end] #select the batch classes by slicing between j * batch_size and (j+1)*batch_size in the shuffled data\n",
    "            G = Gradient(W, X_batch, t_batch) #calculate the gradient of the loss in the batch\n",
    "              \n",
    "            W -= l_rate*G #update the model parameters\n",
    "            norm_G = np.linalg.norm(W)\n",
    "            epoch += 1\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPLETE THE CODE IN THE FOLLOWING CELL, THEN RUN IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPLETE THE FOLLOWING CELL\n",
    "epochs = 30\n",
    "\n",
    "#Training the models with GD, SGD or mini-batch GD\n",
    "\n",
    "#Define the correct value for batch_size \n",
    "### GD: use the entire training set as one batch\n",
    "batch_size = X_train.shape[1] #batch size for GD\n",
    "W_GD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)\n",
    "\n",
    "### SGD: each iteration updates the model parameters using only one randomly selected data point from the training set\n",
    "batch_size = 1 #batch size for SGD\n",
    "W_SGD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)\n",
    "\n",
    "### Mini batch GD: powers of 2 (e.g., 32, 64, 128) or factors of the dataset size are common choices\n",
    "batch_size = 32 #batch size for mini batch GD\n",
    "W_MGD = logisticGD(X_train, Y_train, batch_size = batch_size, epochs = epochs, tol = 1e-5, l_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy GD: 0.85\n",
      "Test Accuracy GD: 0.7\n",
      "Training Accuracy SGD: 0.6416666666666667\n",
      "Test Accuracy SGD: 0.4\n",
      "Training Accuracy mini batch GD: 0.7916666666666666\n",
      "Test Accuracy mini batch GD: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_GD @ (X_train))\n",
    "train_predictions = np.argmax(Y_train_pred, axis=0)\n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true)\n",
    "print(\"Training Accuracy GD:\", train_accuracy)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_GD @ (X_test))\n",
    "test_predictions = np.argmax(Y_test_pred, axis=0)\n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true)\n",
    "print(\"Test Accuracy GD:\", test_accuracy)\n",
    "\n",
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_SGD @ (X_train))\n",
    "train_predictions = np.argmax(Y_train_pred, axis=0)\n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true)\n",
    "print(\"Training Accuracy SGD:\", train_accuracy)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_SGD @ (X_test))\n",
    "test_predictions = np.argmax(Y_test_pred, axis=0)\n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true)\n",
    "print(\"Test Accuracy SGD:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Calculate accuracy on the training set\n",
    "Y_train_pred = softmax(W_MGD @ (X_train))\n",
    "train_predictions = np.argmax(Y_train_pred, axis=0)\n",
    "train_true = np.argmax(Y_train, axis = 0)\n",
    "train_accuracy = np.mean(train_predictions == train_true)\n",
    "print(\"Training Accuracy mini batch GD:\", train_accuracy)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "Y_test_pred = softmax(W_MGD @ (X_test))\n",
    "test_predictions = np.argmax(Y_test_pred, axis=0)\n",
    "test_true = np.argmax(Y_test, axis = 0)\n",
    "test_accuracy = np.mean(test_predictions == test_true)\n",
    "print(\"Test Accuracy mini batch GD:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
